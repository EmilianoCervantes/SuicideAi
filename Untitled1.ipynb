{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "#Regex para expresiones\n",
    "import glob\n",
    "#Encodear palabras\n",
    "import codecs\n",
    "#Lenguaje Natural\n",
    "import nltk\n",
    "#Expresiones regulares\n",
    "import re\n",
    "#Remueve Acentos\n",
    "import unidecode\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from math import log\n",
    "#Para hacer uso de Google Spreadsheets\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archivos = sorted(glob.glob(\"Documents/Escuelas/Tec/Octavo\\ Semestre/Ciencia\\ Cognitiva/SuicideAi/*.txt\"))\n",
    "archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = u\"\"\n",
    "for archivo in archivos:\n",
    "    print(\"Estoy leyendo el archivo {0}\".format(archivo))\n",
    "    with codecs.open(archivo, \"r\", \"utf-8\") as archivo:\n",
    "        corpus_raw += archivo.read()\n",
    "    print(\"El corpus tiene {0} characteres\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming=[]\n",
    "with open('Names.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        naming.append(row['Name'].lower())\n",
    "names=set(naming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpia characteres, mantiene preguntas, acentos y ñ para terminos de contexto\n",
    "toktok = ToktokTokenizer()\n",
    "preposiciones = [\"a\", \"ante\", \"bajo\", \"cabe\", \"con\", \"contra\", \"de\", \"desde\", \"durante\", \"nancys\" ,\"en\", \"entre\", \"hacia\", \"hasta\", \"mediante\", \"para\", \"por\", \"segun\", \"sin\", \"so\", \"sobre\", \"tras\", \"versus\",\"via\"]\n",
    "articulos = [\"el\", \"lo\" ,\"la\", \"alex\" ,\"los\", \"las\", \"esas\",\"esa\",\"es\",\"que\" ,\"nos\", \"tan\", \"estas\",\"ella\",\"misma\" ,\"ello\", \"algo\",\"cosa\",\"pero\", \"como\", \"esta\", \"eres\", \"esas\", \"ha\", \"eh\"]\n",
    "symbols = re.compile(r'(\\W+)', re.U)\n",
    "stop = stopwords.words('spanish')\n",
    "\n",
    "import re\n",
    "\n",
    "contractions = re.compile(r\"'|-|\\\"\")\n",
    "# single character removal\n",
    "singles = re.compile(r'(\\s\\S\\s)', re.I|re.U)\n",
    "# separators (any whitespace)\n",
    "seps = re.compile(r'\\s+')\n",
    "\n",
    "# cleaner (order matters)\n",
    "def clean(text): \n",
    "    text = text.lower()\n",
    "    text = contractions.sub('', text)\n",
    "    text = singles.sub(' ', text)\n",
    "    text = seps.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "# sentence splitter\n",
    "alteos = re.compile(r'([!\\?])')\n",
    "def sentences(l):\n",
    "    l = alteos.sub(r' \\1 .', l).rstrip(\"(\\.)*\\n\")\n",
    "\n",
    "def limpia_impureza(raw):\n",
    "    texto = re.sub(\"[^a-zA-Zñáéíóúü]\",\" \",raw)\n",
    "    texto = texto.lower()\n",
    "    unaccented_string=unidecode.unidecode(clean(texto)).split()\n",
    "    result = list(set(unaccented_string) - set(preposiciones)- set(articulos))\n",
    "    important_words=[]\n",
    "    for word in result:\n",
    "        if word is 'no' or 'si' or word not in stop:\n",
    "            if len(word) > 3 and word not in names:\n",
    "                important_words.append(word)\n",
    "        else:\n",
    "            print(word)\n",
    "    result=' '.join(important_words)\n",
    "    return toktok.tokenize(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        vocabulary.append(limpia_impureza(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 0\n"
     ]
    }
   ],
   "source": [
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "  \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    index = dictionary.get(word, 0)\n",
    "    if index == 0:  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  word_dictionary = dict(zip(dictionary.keys(), dictionary.values()))\n",
    "  return data, count, dictionary, reversed_dictionary, word_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 1083282\n"
     ]
    }
   ],
   "source": [
    "for vocab in vocabulary:\n",
    "    for word in vocab:\n",
    "        with open(\"build.txt\", \"a\") as myfile:\n",
    "            myfile.write(word+\"\\n\")\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    corpus_raw = u\"\"\n",
    "    with codecs.open(filename, \"r\", \"utf-8\") as archivo:\n",
    "        corpus_raw += archivo.read()\n",
    "    return corpus_raw.split()\n",
    "            \n",
    "vocabulario = read_data(\"build.txt\")\n",
    "print('Data size', len(vocabulario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 0], ('cuando', 12025), ('todo', 9716), ('alguien', 9546), ('persona', 7289)]\n",
      "Sample data [29, 743, 696, 103, 287, 1722, 1134, 2595, 27, 104] ['gente', 'dejes', 'jodido', 'menos', 'necesitas', 'hipocresia', 'sangre', 'alarmo', 'nadie', 'estaba']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 50000\n",
    "\n",
    "data, count, dictionary, reverse_dictionary, word_dictionary = build_dataset(\n",
    "    vocabulario, vocabulary_size)\n",
    "del vocabulario  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "  if data_index + span > len(data):\n",
    "    data_index = 0\n",
    "  buffer.extend(data[data_index:data_index + span])\n",
    "  data_index += span\n",
    "  for i in range(batch_size // num_skips):\n",
    "    context_words = [w for w in range(span) if w != skip_window]\n",
    "    words_to_use = random.sample(context_words, num_skips)\n",
    "    for j, context_word in enumerate(words_to_use):\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "    if data_index == len(data):\n",
    "      buffer.extend(data[0:span])\n",
    "      data_index = span\n",
    "    else:\n",
    "      buffer.append(data[data_index])\n",
    "      data_index += 1\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743 dejes -> 696 jodido\n",
      "743 dejes -> 29 gente\n",
      "696 jodido -> 103 menos\n",
      "696 jodido -> 743 dejes\n",
      "103 menos -> 696 jodido\n",
      "103 menos -> 287 necesitas\n",
      "287 necesitas -> 103 menos\n",
      "287 necesitas -> 1722 hipocresia\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-c8546ea7bd9c>:65: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  with tf.name_scope('inputs'):\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    with tf.name_scope('embeddings'):\n",
    "      embeddings = tf.Variable(\n",
    "          tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "      embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    with tf.name_scope('weights'):\n",
    "      nce_weights = tf.Variable(\n",
    "          tf.truncated_normal(\n",
    "              [vocabulary_size, embedding_size],\n",
    "              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    with tf.name_scope('biases'):\n",
    "      nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  # Explanation of the meaning of NCE loss:\n",
    "  #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "  with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weights,\n",
    "            biases=nce_biases,\n",
    "            labels=train_labels,\n",
    "            inputs=embed,\n",
    "            num_sampled=num_sampled,\n",
    "            num_classes=vocabulary_size))\n",
    "\n",
    "  # Add the loss value as a scalar to summary.\n",
    "  tf.summary.scalar('loss', loss)\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                            valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Merge all summaries.\n",
    "  merged = tf.summary.merge_all()\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "  # Create a saver.\n",
    "  saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 : 304.2824401855469\n",
      "Nearest to dormir: cintas, pala, hagas, pala, pala, pala, pala,\n",
      "Nearest to nada: soltar, pala, pala, pala, pala, pala, pala,\n",
      "Nearest to mismo: pala, pala, pala, pala, pala, pala, pala,\n",
      "Nearest to cansado: pala, pala, pala, pala, pala, pala, nuevas,\n",
      "Nearest to estar: termino, fran, pala, pala, rendi, pala, pala,\n",
      "Nearest to cosas: pala, pala, pala, pala, pala, pala, pala,\n",
      "Nearest to bien: pala, pala, pala, suceder, habilidad, pala, pala,\n",
      "Nearest to problemas: pala, pala, pala, pala, pala, cuello, presentas,\n",
      "Nearest to momento: pala, pala, pala, coleccion, pala, pala, pala,\n",
      "Nearest to porque: pala, pala, pala, pala, pala, pala, pala,\n",
      "Nearest to verdad: pala, pala, pala, pala, pala, rajoy, idolos,\n",
      "Nearest to nunca: plazos, jabon, pala, pala, pala, pala, queman,\n",
      "Nearest to quiere: pala, pala, pala, arruinas, pala, pala, pala,\n",
      "Nearest to hace: pala, pala, pala, sien, pala, pala, pala,\n",
      "Nearest to estoy: pala, estaba, pala, pala, pala, pala, pala,\n",
      "Nearest to nadie: ando, pala, pala, pedia, pala, pala, pala,\n",
      "Average loss at step  2000 : 116.57964356422424\n",
      "Average loss at step  4000 : 46.632820870161055\n",
      "Average loss at step  6000 : 27.140446909189226\n",
      "Average loss at step  8000 : 17.016531152486802\n",
      "Average loss at step  10000 : 11.578567787706852\n",
      "Nearest to dormir: cintas, hagas, acudir, moleste, deseos, compararte, muchisimo,\n",
      "Nearest to nada: soltar, pesadilla, reemplazarme, hija, descargarme, voluntaria, ryan,\n",
      "Nearest to mismo: ventaja, instalar, sacar, alejes, ojeras, preguntandote, elgiendome,\n",
      "Nearest to cansado: eterna, atado, imaginas, nuevas, jugamos, incierto, ascazo,\n",
      "Nearest to estar: termino, rendi, estupidez, sonria, escenario, note, precio,\n",
      "Nearest to cosas: viajar, sufren, levantar, lucir, pala, consuela, ahogaba,\n",
      "Nearest to bien: andas, miradas, consumiendo, guardas, verdadero, oido, afiliado,\n",
      "Nearest to problemas: olvida, insepulto, desee, digas, pulseras, cielo, cuello,\n",
      "Nearest to momento: intento, coleccion, millones, sabre, asociacion, dimision, desear,\n",
      "Nearest to porque: sentirme, coloquio, cansare, alex, merezco, antiguas, firmo,\n",
      "Nearest to verdad: replay, rajoy, idolos, colocar, felicidad, amancio, rompiste,\n",
      "Nearest to nunca: jabon, subido, queman, redondeando, plazos, vecino, justo,\n",
      "Nearest to quiere: incomodo, lavando, arruinas, valora, resto, lloro, nirvana,\n",
      "Nearest to hace: sien, desahogado, modalidad, retiro, arena, tenerte, querian,\n",
      "Nearest to estoy: estaba, espuma, salve, acompanaron, imaginas, ponen, confirma,\n",
      "Nearest to nadie: ando, llevando, aguantas, prometido, pedia, remedios, raeinforma,\n",
      "Average loss at step  12000 : 8.546961554527282\n",
      "Average loss at step  14000 : 7.247090960204601\n",
      "Average loss at step  16000 : 7.358105044603348\n",
      "Average loss at step  18000 : 5.698048623263836\n",
      "Average loss at step  20000 : 3.8136862522065638\n",
      "Nearest to dormir: hagas, cintas, acudir, asesinar, deseos, moleste, compararte,\n",
      "Nearest to nada: soltar, pesadilla, reemplazarme, descargarme, madina, voluntaria, hija,\n",
      "Nearest to mismo: ventaja, sacar, vacio, instalar, ojeras, estresare, anterior,\n",
      "Nearest to cansado: eterna, atado, imaginas, jugamos, ascazo, incierto, detallista,\n",
      "Nearest to estar: termino, estupidez, sonria, escenario, cantando, espere, soportas,\n",
      "Nearest to cosas: levantar, viajar, consuela, estresarme, coran, sufren, comenzado,\n",
      "Nearest to bien: andas, consumiendo, verdadero, digan, miradas, oido, guardas,\n",
      "Nearest to problemas: insepulto, olvida, desee, presentas, cuello, habilidad, digas,\n",
      "Nearest to momento: millones, intento, sabre, dimision, asociacion, desear, escondo,\n",
      "Nearest to porque: cansare, sentirme, antiguas, coloquio, cambiante, firmo, viaje,\n",
      "Nearest to verdad: rajoy, colocar, replay, idolos, amancio, resultan, rompiste,\n",
      "Nearest to nunca: jabon, subido, vecino, redondeando, odias, plazos, justo,\n",
      "Nearest to quiere: incomodo, valora, arruinas, lavando, resto, ganado, gritar,\n",
      "Nearest to hace: sien, desahogado, arena, retiro, metiendo, modalidad, medico,\n",
      "Nearest to estoy: imaginas, ponen, acompanaron, salve, espuma, alonso, replay,\n",
      "Nearest to nadie: ando, llevando, aguantas, rayas, raeinforma, prometido, desarrollando,\n",
      "Average loss at step  22000 : 3.222770705372095\n",
      "Average loss at step  24000 : 2.8236224786937236\n",
      "Average loss at step  26000 : 2.5592926315069198\n",
      "Average loss at step  28000 : 2.345055299639702\n",
      "Average loss at step  30000 : 2.082544087678194\n",
      "Nearest to dormir: hagas, acudir, cintas, deseos, asesinar, moleste, invitan,\n",
      "Nearest to nada: soltar, pesadilla, madina, reemplazarme, descargarme, voluntaria, elgiendome,\n",
      "Nearest to mismo: ventaja, anterior, ojeras, vacio, estresare, sacar, pinte,\n",
      "Nearest to cansado: atado, eterna, jugamos, incierto, imaginas, loaf, acurrucarme,\n",
      "Nearest to estar: termino, sonria, estupidez, espere, besarte, tuits, cantando,\n",
      "Nearest to cosas: levantar, viajar, comenzado, coran, consuela, estresarme, edificios,\n",
      "Nearest to bien: andas, verdadero, consumiendo, miradas, digan, inversa, afiliado,\n",
      "Nearest to problemas: insepulto, olvida, desee, reinicia, presentas, habilidad, juan,\n",
      "Nearest to momento: intento, millones, tropezarnos, sabre, dimision, mantenia, suma,\n",
      "Nearest to porque: cansare, firmo, antiguas, borda, cambiante, sentirme, viaje,\n",
      "Nearest to verdad: rajoy, colocar, replay, resultan, amancio, idolos, rompiste,\n",
      "Nearest to nunca: subido, vecino, redondeando, jabon, odias, consumes, salimos,\n",
      "Nearest to quiere: ganado, gritar, lavando, incomodo, valora, reparado, entretenimiento,\n",
      "Nearest to hace: sien, arena, retiro, medico, metiendo, desahogado, modalidad,\n",
      "Nearest to estoy: imaginas, ponen, salve, espuma, acompanaron, oscuro, replay,\n",
      "Nearest to nadie: ando, rayas, hadas, desarrollando, remedios, necesitado, aguantas,\n",
      "Average loss at step  32000 : 4.375399300098419\n",
      "Average loss at step  34000 : 4.179510455727577\n",
      "Average loss at step  36000 : 2.533506143093109\n",
      "Average loss at step  38000 : 1.9944005547761916\n",
      "Average loss at step  40000 : 1.9665530243813991\n",
      "Nearest to dormir: hagas, acudir, asesinar, cintas, perfil, guardarme, deseos,\n",
      "Nearest to nada: pesadilla, soltar, descargarme, madina, reemplazarme, inflarlo, zombi,\n",
      "Nearest to mismo: ventaja, anterior, estresare, ojeras, sacar, pinte, vacio,\n",
      "Nearest to cansado: atado, eterna, jugamos, imaginas, esperan, loaf, incierto,\n",
      "Nearest to estar: termino, sonria, estupidez, escenario, aceptado, espere, besarte,\n",
      "Nearest to cosas: levantar, comenzado, coran, consuela, tardio, estresarme, viajar,\n",
      "Nearest to bien: verdadero, andas, consumiendo, digan, miradas, oido, inversa,\n",
      "Nearest to problemas: insepulto, desee, presentas, olvida, habilidad, cuantos, insistente,\n",
      "Nearest to momento: millones, suma, mantenia, tropezarnos, propongo, sabre, intento,\n",
      "Nearest to porque: cansare, firmo, borda, cambiante, antiguas, viaje, sentirme,\n",
      "Nearest to verdad: rajoy, colocar, resultan, amancio, replay, batalla, enviaste,\n",
      "Nearest to nunca: odias, subido, redondeando, jabon, vecino, salimos, consumes,\n",
      "Nearest to quiere: gritar, ganado, valora, reparado, entretenimiento, pongo, lavando,\n",
      "Nearest to hace: sien, arena, retiro, medico, desahogado, echase, metiendo,\n",
      "Nearest to estoy: imaginas, ponen, salve, espuma, acompanaron, oscuro, replay,\n",
      "Nearest to nadie: ando, hadas, rayas, armartela, remedios, farmaceutica, rico,\n",
      "Average loss at step  42000 : 1.909763056218624\n",
      "Average loss at step  44000 : 1.8646770071983338\n",
      "Average loss at step  46000 : 1.7862029855102302\n",
      "Average loss at step  48000 : 2.9298163617551327\n",
      "Average loss at step  50000 : 3.6147975959777834\n",
      "Nearest to dormir: hagas, asesinar, cuentes, cintas, pasiva, acudir, perfil,\n",
      "Nearest to nada: pesadilla, soltar, madina, descargarme, cumplieran, zombi, planetas,\n",
      "Nearest to mismo: sacar, estresare, ventaja, anterior, reflexionar, fallan, ojeras,\n",
      "Nearest to cansado: jugamos, eterna, atado, esperan, loaf, cavando, curas,\n",
      "Nearest to estar: sonria, termino, medicos, quitarle, aceptado, escenario, bien,\n",
      "Nearest to cosas: levantar, coran, incapaz, compre, noventa, aprovechate, novia,\n",
      "Nearest to bien: digan, consumiendo, miradas, andas, inversa, verdadero, guardas,\n",
      "Nearest to problemas: insepulto, presentas, habilidad, desee, gritarle, insistente, juan,\n",
      "Nearest to momento: suma, millones, escucharme, tropezarnos, sabre, propongo, movil,\n",
      "Nearest to porque: ofrecen, cambiante, quedate, cansare, veces, guardamos, adolescentes,\n",
      "Nearest to verdad: rajoy, amancio, colocar, resultan, replay, seis, enviaste,\n",
      "Nearest to nunca: llevate, salimos, subido, jabon, consumes, odias, cuadraditos,\n",
      "Nearest to quiere: reparado, valora, gritar, ganado, quedaras, pongo, convirtieron,\n",
      "Nearest to hace: sien, arena, retiro, medico, muestra, desahogado, contrario,\n",
      "Nearest to estoy: salve, imaginas, ponen, olvidalo, replay, conseguiras, anterior,\n",
      "Nearest to nadie: ando, remedios, hadas, frustracion, farmaceutica, armartela, rico,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  52000 : 3.3270055504143237\n",
      "Average loss at step  54000 : 1.8535230632126332\n",
      "Average loss at step  56000 : 1.822568397641182\n",
      "Average loss at step  58000 : 1.8002398593276738\n",
      "Average loss at step  60000 : 1.784830445215106\n",
      "Nearest to dormir: asesinar, acudir, guardarme, perfil, hagas, pasiva, minima,\n",
      "Nearest to nada: pesadilla, soltar, inflarlo, zombi, cumplieran, madina, descargarme,\n",
      "Nearest to mismo: anterior, ventaja, estresare, ojeras, pinte, reflexionar, fallan,\n",
      "Nearest to cansado: atado, jugamos, eterna, febrero, loaf, curas, esperan,\n",
      "Nearest to estar: termino, sonria, estupidez, internacional, desee, escenario, tuits,\n",
      "Nearest to cosas: levantar, tardio, comenzado, coran, incapaz, consuela, simulacro,\n",
      "Nearest to bien: miradas, verdadero, andas, digan, consumiendo, inversa, esperaron,\n",
      "Nearest to problemas: insepulto, presentas, insistente, habilidad, desee, cuantos, juan,\n",
      "Nearest to momento: suma, mantenia, millones, tropezarnos, leche, propongo, escucharme,\n",
      "Nearest to porque: firmo, cansare, borda, quedate, cambiante, punta, adolescentes,\n",
      "Nearest to verdad: colocar, resultan, amancio, rajoy, enviaste, replay, seis,\n",
      "Nearest to nunca: vecino, odias, salimos, redondeando, llevate, jabon, consumes,\n",
      "Nearest to quiere: gritar, quedaras, ganado, perfecto, reparado, entretenimiento, valora,\n",
      "Nearest to hace: sien, arena, retiro, medico, contrario, desahogado, observo,\n",
      "Nearest to estoy: imaginas, ponen, salve, oscuro, espuma, olvidalo, acompanaron,\n",
      "Nearest to nadie: ando, hadas, remedios, armartela, farmaceutica, frustracion, cuide,\n",
      "Average loss at step  62000 : 1.6923101446926594\n",
      "Average loss at step  64000 : 1.7053471833765508\n",
      "Average loss at step  66000 : 3.745018740326166\n",
      "Average loss at step  68000 : 3.472857100725174\n",
      "Average loss at step  70000 : 2.053943527162075\n",
      "Nearest to dormir: asesinar, perfil, hagas, acudir, guardarme, deseas, cuentes,\n",
      "Nearest to nada: pesadilla, soltar, cumplieran, zombi, inflarlo, madina, descargarme,\n",
      "Nearest to mismo: anterior, estresare, ventaja, fallan, reflexionar, ojeras, tematica,\n",
      "Nearest to cansado: jugamos, eterna, esperan, ninguna, cavando, febrero, curas,\n",
      "Nearest to estar: sonria, termino, aceptado, estupidez, desee, debe, besarte,\n",
      "Nearest to cosas: levantar, coran, tardio, aprovechate, noventa, consuela, incapaz,\n",
      "Nearest to bien: digan, miradas, andas, verdadero, consumiendo, todos, alguien,\n",
      "Nearest to problemas: insepulto, presentas, gritarle, insistente, habilidad, desee, juan,\n",
      "Nearest to momento: suma, escucharme, millones, mantenia, propongo, sabre, tropezarnos,\n",
      "Nearest to porque: persona, firmo, borda, nunca, adolescentes, cambiante, cansare,\n",
      "Nearest to verdad: amancio, colocar, resultan, rajoy, seis, enviaste, espanolas,\n",
      "Nearest to nunca: odias, llevate, salimos, jabon, vecino, hacer, redondeando,\n",
      "Nearest to quiere: perfecto, quedaras, reparado, gritar, ganado, distintas, valora,\n",
      "Nearest to hace: sien, arena, retiro, medico, acumula, echase, siempre,\n",
      "Nearest to estoy: imaginas, ponen, salve, olvidalo, oscuro, alonso, acompanaron,\n",
      "Nearest to nadie: armartela, ando, hadas, cuide, remedios, honestos, perez,\n",
      "Average loss at step  72000 : 1.7914650057852268\n",
      "Average loss at step  74000 : 1.7711196717917919\n",
      "Average loss at step  76000 : 1.7540141455978155\n",
      "Average loss at step  78000 : 1.647139988362789\n",
      "Average loss at step  80000 : 1.675800921946764\n",
      "Nearest to dormir: asesinar, perfil, guardarme, acudir, minima, botes, pasiva,\n",
      "Nearest to nada: pesadilla, inflarlo, cumplieran, soltar, zombi, escuchen, madina,\n",
      "Nearest to mismo: anterior, estresare, ventaja, ojeras, reflexionar, frio, tiraron,\n",
      "Nearest to cansado: febrero, curas, jugamos, eterna, ninguna, esperan, loaf,\n",
      "Nearest to estar: sonria, termino, tuits, aceptado, internacional, desee, debe,\n",
      "Nearest to cosas: levantar, tardio, comenzado, coran, simulacro, noventa, incapaz,\n",
      "Nearest to bien: miradas, verdadero, consumiendo, andas, digan, esperaron, inversa,\n",
      "Nearest to problemas: insepulto, presentas, insistente, gritarle, habilidad, saludo, desee,\n",
      "Nearest to momento: mantenia, suma, tropezarnos, escucharme, propongo, leche, espacio,\n",
      "Nearest to porque: firmo, borda, cambiante, punta, cansare, frenando, viaje,\n",
      "Nearest to verdad: resultan, colocar, amancio, rajoy, enviaste, espanolas, seis,\n",
      "Nearest to nunca: salimos, odias, vecino, redondeando, yace, desconocen, plazos,\n",
      "Nearest to quiere: quedaras, perfecto, gritar, ganado, distintas, reparado, entretenimiento,\n",
      "Nearest to hace: sien, retiro, arena, medico, observo, echase, acumula,\n",
      "Nearest to estoy: imaginas, ponen, salve, oscuro, olvidalo, amistades, acompanaron,\n",
      "Nearest to nadie: ando, remedios, armartela, honestos, cuide, hadas, ferb,\n",
      "Average loss at step  82000 : 2.9501604474782943\n",
      "Average loss at step  84000 : 2.978454281151295\n",
      "Average loss at step  86000 : 2.8392786485403776\n",
      "Average loss at step  88000 : 1.8045618251860143\n",
      "Average loss at step  90000 : 1.7691787532269954\n",
      "Nearest to dormir: asesinar, guardarme, perfil, acudir, botes, risa, minima,\n",
      "Nearest to nada: pesadilla, inflarlo, zombi, cumplieran, soltar, madina, escuchen,\n",
      "Nearest to mismo: reflexionar, anterior, ventaja, estresare, fallan, ojeras, tiraron,\n",
      "Nearest to cansado: febrero, jugamos, curas, ninguna, esperan, eterna, cavando,\n",
      "Nearest to estar: sonria, termino, debe, tuits, llenan, desee, besarte,\n",
      "Nearest to cosas: levantar, coran, tardio, simulacro, comenzado, noventa, consuela,\n",
      "Nearest to bien: miradas, digan, verdadero, consumiendo, esperaron, andas, destruyes,\n",
      "Nearest to problemas: insepulto, presentas, gritarle, insistente, saludo, habilidad, cuello,\n",
      "Nearest to momento: escucharme, suma, mantenia, millones, tropezarnos, mintiendo, candy,\n",
      "Nearest to porque: firmo, borda, comparandome, cambiante, adolescentes, guste, punta,\n",
      "Nearest to verdad: amancio, resultan, colocar, enviaste, rajoy, seis, espanolas,\n",
      "Nearest to nunca: odias, vecino, salimos, yace, llevate, redondeando, jabon,\n",
      "Nearest to quiere: perfecto, quedaras, reinicia, ganado, gritar, reparado, distintas,\n",
      "Nearest to hace: sien, arena, retiro, observo, contrario, semana, echase,\n",
      "Nearest to estoy: imaginas, ponen, salve, oscuro, olvidalo, anterior, amistades,\n",
      "Nearest to nadie: armartela, honestos, perez, colgado, rico, hadas, suicidarse,\n",
      "Average loss at step  92000 : 1.7371018439233303\n",
      "Average loss at step  94000 : 1.6217196683958173\n",
      "Average loss at step  96000 : 1.676375637203455\n",
      "Average loss at step  98000 : 1.7486068372502923\n",
      "Average loss at step  100000 : 3.3536186918914317\n",
      "Nearest to dormir: asesinar, perfil, deseas, cuentes, alguien, pasiva, dana,\n",
      "Nearest to nada: inflarlo, pesadilla, cumplieran, zombi, madina, soltar, escuchen,\n",
      "Nearest to mismo: reflexionar, frio, fallan, estresare, hablo, ventaja, anterior,\n",
      "Nearest to cansado: jugamos, curas, esperan, febrero, cavando, eterna, nooo,\n",
      "Nearest to estar: sonria, quitarle, desastre, medicos, aburrida, consistian, aceptado,\n",
      "Nearest to cosas: coran, incapaz, levantar, noventa, aprovechate, compre, simulacro,\n",
      "Nearest to bien: votando, digan, miradas, verdadero, consumiendo, esperaron, inversa,\n",
      "Nearest to problemas: insepulto, presentas, gritarle, saludo, recogiendo, insistente, cuello,\n",
      "Nearest to momento: escucharme, suma, espacio, refleja, empece, mantenia, personas,\n",
      "Nearest to porque: envuelto, cambiante, adolescentes, veces, quedate, murmurando, siempre,\n",
      "Nearest to verdad: amancio, colocar, resultan, rajoy, seis, espanolas, enviaste,\n",
      "Nearest to nunca: todos, salimos, cuando, llevate, odias, hacer, tiburon,\n",
      "Nearest to quiere: quedaras, reinicia, reparado, perfecto, valora, gritar, pongo,\n",
      "Nearest to hace: sien, observo, arena, preocuparme, contrario, permites, medico,\n",
      "Nearest to estoy: salve, ponen, anterior, amistades, olvidalo, ahora, conseguiras,\n",
      "Nearest to nadie: armartela, honestos, perez, hadas, veias, cuide, rico,\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "# Give a folder path as an argument with '--log_dir' to save\n",
    "# TensorBoard summaries. Default is a log folder in current directory.\n",
    "current_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--log_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'log'),\n",
    "    help='The log directory for TensorBoard summaries.')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Create the directory for TensorBoard variables if there is not.\n",
    "if not os.path.exists(FLAGS.log_dir):\n",
    "    os.makedirs(FLAGS.log_dir)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips,skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "            [optimizer, merged, loss],\n",
    "            feed_dict=feed_dict,\n",
    "            run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ':', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 7  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    try:\n",
    "                        close_word = reverse_dictionary[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                    except KeyError:\n",
    "                        close_word = reverse_dictionary[len(reverse_dictionary)-1]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            \n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "        \n",
    "    # Write corresponding labels for the embeddings.\n",
    "    with open(FLAGS.log_dir + '/metadata.tsv', 'w') as f:\n",
    "        for i in xrange(vocabulary_size):\n",
    "            try:\n",
    "                f.write(reverse_dictionary[i] + '\\n')\n",
    "            except KeyError:\n",
    "                break\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(FLAGS.log_dir, 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "# Function to draw visualization of distance between embeddings.\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,xy=(x, y),xytext=(5, 2),textcoords='offset points',ha='right',va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "try:\n",
    "    # pylint: disable=g-import-not-at-top\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n",
    "    \n",
    "except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNK', 'cuando', 'todo']\n",
      "[[ 5.63721322e-02  1.37281328e-01  6.68083578e-02  3.10999006e-02\n",
      "  -1.29610300e-02  1.30070299e-01  4.46993336e-02 -1.52785957e-01\n",
      "  -2.01562028e-02  9.42291543e-02  1.48578599e-01 -8.40875730e-02\n",
      "   4.40414511e-02  1.15703858e-01 -8.48140493e-02 -1.35610491e-01\n",
      "  -1.35987177e-01 -1.25011221e-01 -3.91819291e-02 -1.37329429e-01\n",
      "   1.43736387e-02 -1.08650148e-01  6.85817301e-02 -3.03956755e-02\n",
      "  -6.04532892e-03 -8.99951905e-02  7.10171908e-02  7.72865415e-02\n",
      "  -1.12918012e-01 -9.09323916e-02  9.37023908e-02 -1.33120254e-01\n",
      "  -5.96815161e-02  2.24603880e-02 -2.61536539e-02 -3.39178480e-02\n",
      "   1.22634321e-01 -1.76194124e-02  6.00935332e-02 -1.37340546e-01\n",
      "   6.61149994e-02  6.07540384e-02  1.20361455e-01  7.77095705e-02\n",
      "  -3.19932005e-04  1.49236068e-01 -7.87371024e-02 -1.13334864e-01\n",
      "  -4.65366198e-03 -1.32439792e-01 -1.54189438e-01 -7.26279020e-02\n",
      "   3.71317789e-02 -7.25763515e-02  6.45924360e-02  1.03671871e-01\n",
      "   1.08564653e-01  5.08054998e-03 -2.54560225e-02 -1.18583359e-01\n",
      "  -4.15107645e-02  9.48199779e-02  1.29244894e-01  8.54182094e-02\n",
      "   9.83403549e-02  1.20863114e-02 -1.22726895e-02  1.43943101e-01\n",
      "  -9.74093303e-02  6.68771788e-02  1.12255299e-02  3.06747351e-02\n",
      "  -9.84307528e-02 -8.24767500e-02 -9.92920622e-03 -9.84868705e-02\n",
      "  -6.26313761e-02  1.31943105e-02  1.15983628e-01 -9.76549014e-02\n",
      "   1.35805145e-01 -2.38201749e-02  8.17602314e-03 -1.14760064e-02\n",
      "   5.36280945e-02 -1.08148143e-01  1.38041675e-01  4.38966192e-02\n",
      "  -1.50106326e-01  1.80001464e-02  6.07195348e-02  1.21026754e-01\n",
      "   1.13849454e-01 -8.16700980e-02  1.52287334e-01  1.26182199e-01\n",
      "   3.05891708e-02 -7.21153691e-02 -5.72569259e-02  1.24707920e-02\n",
      "   7.51770884e-02  6.14239834e-02  5.12662195e-02  1.23830244e-01\n",
      "   3.81244309e-02 -4.52069901e-02 -1.26758441e-01 -1.11224771e-01\n",
      "  -1.30384803e-01 -1.26252592e-01  1.18214879e-02  5.39691560e-02\n",
      "   1.22559622e-01 -2.90352907e-02  8.40519816e-02  9.81300026e-02\n",
      "  -8.16126354e-03 -4.00296748e-02  4.21365798e-02  1.33491069e-01\n",
      "  -1.56771138e-01 -1.54717550e-01 -3.09783337e-03 -3.65989842e-02\n",
      "  -7.42886623e-05 -4.71256487e-02 -2.90073808e-02  5.90355843e-02]\n",
      " [ 5.15790023e-02  7.11371237e-03  5.64905703e-02 -4.00577672e-02\n",
      "  -4.46446501e-02 -6.64297417e-02 -7.42904618e-02  1.53646275e-01\n",
      "   2.17479870e-01 -7.23782228e-03 -4.78406772e-02 -9.32040587e-02\n",
      "   7.58071616e-02 -3.77774313e-02  7.88166225e-02 -1.15825608e-01\n",
      "  -5.84756359e-02 -3.18953395e-02  1.12369984e-01  1.07519343e-01\n",
      "  -8.00588429e-02  2.02059641e-01 -1.34472013e-01 -7.98644274e-02\n",
      "  -2.95874663e-02  2.53588557e-02  9.02302712e-02 -1.30172685e-01\n",
      "   2.03012209e-02  1.01271365e-02  4.04457636e-02 -5.76384924e-02\n",
      "   1.59856603e-02 -1.54387012e-01 -2.07146868e-01  5.46335801e-02\n",
      "   1.21716522e-01 -1.05145096e-03 -2.90459488e-02 -7.17010396e-03\n",
      "   3.74018438e-02 -1.08618196e-02 -1.21996962e-01 -1.00064293e-01\n",
      "  -1.39989993e-02  7.48546841e-03  1.39444917e-01 -4.52121310e-02\n",
      "  -8.64777789e-02 -2.17435770e-02 -2.82217450e-02 -4.79173288e-02\n",
      "  -3.34355459e-02 -2.01162562e-01  5.48748598e-02 -3.02634630e-02\n",
      "   9.81628373e-02 -1.72696915e-02  2.35411860e-02  2.38785371e-02\n",
      "   3.63584259e-03  4.04857807e-02  6.44587204e-02 -1.72415018e-01\n",
      "   3.23984623e-02 -7.80743435e-02  1.90630846e-03 -5.35472520e-02\n",
      "   2.09085897e-01 -1.02631427e-01 -6.61846101e-02  9.30345990e-03\n",
      "  -2.87734196e-02  1.30298898e-01  1.20380558e-02  1.31651819e-01\n",
      "  -2.28175372e-02  4.29039672e-02  8.33346229e-03  9.56057608e-02\n",
      "  -1.13746539e-01  1.77481860e-01  1.74264982e-02  7.32506886e-02\n",
      "  -5.86114312e-03 -5.22098690e-03  8.44871774e-02 -7.71724954e-02\n",
      "  -1.14555778e-02  6.22282326e-02 -1.33436278e-01  1.70064960e-02\n",
      "  -8.20571259e-02 -3.59844454e-02  6.03143387e-02 -2.07055300e-01\n",
      "   7.38463104e-02  1.05981596e-01  1.14260793e-01 -1.82899460e-02\n",
      "  -3.54385860e-02 -9.38010663e-02  5.01245670e-02 -4.35310118e-02\n",
      "  -9.02456045e-02 -7.81334043e-02 -1.92070603e-02  9.76684690e-03\n",
      "  -5.17952256e-02  5.15254512e-02 -6.02592677e-02 -1.17312402e-01\n",
      "   7.23677501e-02  2.17675000e-01  9.07233208e-02 -2.00922340e-02\n",
      "   1.21376075e-01  1.04201131e-01 -6.18776307e-02 -1.40959263e-01\n",
      "   8.04060251e-02  6.57281950e-02 -9.72966105e-02 -7.94996321e-02\n",
      "  -1.39818564e-01  4.24539000e-02 -5.03685363e-02  6.29591644e-02]\n",
      " [ 7.82970712e-02  3.34664881e-02  1.16060965e-01 -1.00920431e-01\n",
      "  -1.24091163e-01  6.46882281e-02 -6.08085208e-02  4.72281799e-02\n",
      "   6.88803643e-02  6.96865842e-02 -1.22666150e-01 -5.68673350e-02\n",
      "  -1.07577272e-01 -1.80579141e-01  7.15021715e-02  2.64627412e-02\n",
      "  -2.16508768e-02  1.43582448e-01  7.59816766e-02  3.26837637e-02\n",
      "  -1.40452096e-02  2.65163202e-02 -4.73508760e-02 -1.14750884e-01\n",
      "   7.87103325e-02 -1.42851278e-01  3.98962349e-02 -3.41483089e-03\n",
      "   9.54649225e-02 -4.98476103e-02  6.09196648e-02 -8.25028718e-02\n",
      "   9.52159911e-02 -2.41474554e-01 -4.42044139e-02  1.75110832e-01\n",
      "   7.68120494e-03  2.69649867e-02  1.93942562e-02  7.04924837e-02\n",
      "   3.26187350e-02  8.13051984e-02 -7.85522535e-02  2.28111763e-02\n",
      "  -1.63230281e-02  8.61871317e-02  1.87893704e-01  1.94317903e-02\n",
      "  -8.00857022e-02  4.65632044e-02 -1.37145057e-01  1.13891110e-01\n",
      "   1.19525798e-01 -1.50699884e-01  5.78669086e-02 -8.98896307e-02\n",
      "   1.18059784e-01 -1.23678528e-01  4.94205346e-03  4.41270880e-02\n",
      "   1.50583327e-01  8.82447045e-03 -2.36772504e-02 -1.19092576e-01\n",
      "   4.09368314e-02 -9.44229215e-02  8.64690766e-02 -5.76733425e-02\n",
      "   3.06884646e-02  8.19819719e-02 -1.02172755e-01  3.25409546e-02\n",
      "  -6.23809062e-02  6.53007999e-02 -4.24122773e-02 -1.81541573e-02\n",
      "   1.71082076e-02  2.32548686e-03  7.80065283e-02 -1.10482424e-02\n",
      "  -3.38759348e-02  6.26386330e-02  1.60963669e-01  5.19461744e-02\n",
      "   1.26119256e-01  5.53310215e-02 -5.61474413e-02  6.85058907e-02\n",
      "   7.28061050e-02  1.89421384e-03 -1.57927349e-02  1.90346003e-01\n",
      "  -1.02797768e-03 -5.67750782e-02  6.37074187e-02 -1.91125363e-01\n",
      "   4.48019290e-03  4.38079126e-02  6.01833779e-03 -2.79448815e-02\n",
      "   2.66910065e-02 -2.25779161e-01  5.03327884e-03 -2.58499943e-02\n",
      "  -5.50427176e-02  5.06240688e-02  1.08842090e-01 -1.90359205e-01\n",
      "   3.08286957e-02  8.82027224e-02  2.07854118e-02  4.34634797e-02\n",
      "  -1.15805529e-01  1.54721126e-01  6.47766050e-03 -7.19657764e-02\n",
      "   6.63158670e-02 -2.71973312e-02  6.98665753e-02 -1.99608847e-01\n",
      "  -7.73079973e-03  4.72005829e-02 -1.67378485e-01 -2.03439873e-02\n",
      "  -1.78936459e-02 -2.32744012e-02 -4.52751629e-02  2.07431819e-02]]\n"
     ]
    }
   ],
   "source": [
    "plot_only = 3\n",
    "labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "print(labels)\n",
    "print(final_embeddings[:plot_only, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name('Credenciales.json', scope)\n",
    "\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "sht1 = gc.open_by_url('https://docs.google.com/spreadsheets/d/1j5uZP0eo4a9FdhM9sKHxb3AVObnsOgB0BBz3lLzOlDI/edit#gid=0')\n",
    "\n",
    "ws = sht1.get_worksheet(0)\n",
    "ws2 = sht1.get_worksheet(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mierda!, es que posta.. Me gustabas mucho ... Pero solo soy un cero a la izquierda...\n",
      "gustabas -> 0\n",
      "izquierda -> 0.009934629631231928 -> 0.9900653703687681\n",
      "mucho -> 0.005601306948278761 -> 0.9943986930517212\n",
      "posta -> 0.012234785302780438 -> 0.9877652146972196\n",
      "cero -> 0.022558496170177023 -> 0.977441503829823\n",
      "mierda -> 0.008338760218180141 -> 0.9916612397818199\n",
      "El porcentaje de suicidarte es de: 0.45\n",
      "No puedo seguir con esto.... No cuando, no siento justo querer a alguien, cuando no me quiero a mi misma...\n",
      "siento -> 0.005646026751378486 -> 0.9943539732486215\n",
      "puedo -> 0.020667304594098823 -> 0.9793326954059012\n",
      "alguien -> 0.02365864252078609 -> 0.9763413574792139\n",
      "cuando -> -0.01999558494981102 -> 1.019995584949811\n",
      "esto -> -0.015585149779724361 -> 1.0155851497797244\n",
      "seguir -> -0.0021855823524674634 -> 1.0021855823524675\n",
      "querer -> 0.028522141997768813 -> 0.9714778580022312\n",
      "quiero -> 0.0568789449880569 -> 0.9431210550119431\n",
      "El porcentaje de suicidarte es de: 0.54\n",
      "No se que pasa.... Solo tengo en claro que esto no quería que sucediera y que ahora necesito km de distancia...\n",
      "necesito -> 0.015715957608222197 -> 0.9842840423917778\n",
      "distancia -> -0.00446537017086257 -> 1.0044653701708626\n",
      "esto -> -0.015585149779724361 -> 1.0155851497797244\n",
      "ahora -> 0.04076098243109527 -> 0.9592390175689047\n",
      "pasa -> 0.013608821417506078 -> 0.9863911785824939\n",
      "tengo -> 0.0227931054672581 -> 0.9772068945327419\n",
      "sucediera -> 0.006630461321492476 -> 0.9933695386785075\n",
      "queria -> 0.012007147313909172 -> 0.9879928526860908\n",
      "El porcentaje de suicidarte es de: 0.52\n",
      "Vida de mierda...\n",
      "mierda -> 0.008338760218180141 -> 0.9916612397818199\n",
      "El porcentaje de suicidarte es de: 0.39\n",
      "RT @eldiariodedross: Cuando tu mamá te dice lo apuesto que eres. https://t.co/HPE5hKU0BK\n",
      "dice -> 0.0009117431915228735 -> 0.9990882568084771\n",
      "cuando -> -0.01999558494981102 -> 1.019995584949811\n",
      "apuesto -> 0\n",
      "https -> 0.0004830377929110341 -> 0.999516962207089\n",
      "eldiariodedross -> 0\n",
      "El porcentaje de suicidarte es de: -0.18\n",
      "Me odio, por el simple hecho de existir ...\n",
      "existir -> 0.024007932941913168 -> 0.9759920670580868\n",
      "simple -> 0.020499554835964773 -> 0.9795004451640352\n",
      "odio -> 0.020479341668647066 -> 0.9795206583313529\n",
      "hecho -> 0.013048686217047134 -> 0.9869513137829529\n",
      "El porcentaje de suicidarte es de: 0.75\n",
      "@LuisArredondoP1 Jajajaja te lo dijimos ....\n",
      "Te quiero llamar, pero me da vergüenza de no saber que decirte o traba… https://t.co/gl7FXEYNwG\n",
      "llamar -> 0.03456423944578546 -> 0.9654357605542145\n",
      "fxeynwg -> 0\n",
      "dijimos -> 0.01483111978626539 -> 0.9851688802137346\n",
      "jajajaja -> 0\n",
      "decirte -> 0.0155509284761024 -> 0.9844490715238976\n",
      "https -> 0.0004830377929110341 -> 0.999516962207089\n",
      "verguenza -> -0.008200802633950843 -> 1.0082008026339508\n",
      "traba -> 0\n",
      "quiero -> 0.0568789449880569 -> 0.9431210550119431\n",
      "luisarredondop -> 0\n",
      "El porcentaje de suicidarte es de: 0.52\n",
      "Fingir que no duele...\n",
      "Duele el doble.\n",
      "duele -> 0.02743483250492318 -> 0.9725651674950768\n",
      "doble -> 0.02036936486143759 -> 0.9796306351385624\n",
      "fingir -> 0.007411831234492183 -> 0.9925881687655078\n",
      "El porcentaje de suicidarte es de: 0.73\n",
      "Dormir....\n",
      "dormir -> 0.017313635548248385 -> 0.9826863644517516\n",
      "El porcentaje de suicidarte es de: 0.70\n",
      "Hoy fue un día bastante descordinado , literal, no sé bien lo que hago.. A parte de que estoy muy cansada.. Sol quiero dormir y dormir....\n",
      "bastante -> -0.009877661029747742 -> 1.0098776610297477\n",
      "hago -> 0.016784162624389865 -> 0.9832158373756101\n",
      "dormir -> 0.017313635548248385 -> 0.9826863644517516\n",
      "estoy -> -0.004200413642365675 -> 1.0042004136423657\n",
      "literal -> 0\n",
      "bien -> 0.002065609393262946 -> 0.997934390606737\n",
      "descordinado -> 0\n",
      "cansada -> -0.01794251756950871 -> 1.0179425175695087\n",
      "quiero -> 0.0568789449880569 -> 0.9431210550119431\n",
      "parte -> 0.017110590662866798 -> 0.9828894093371332\n",
      "El porcentaje de suicidarte es de: 0.37\n",
      "Odio las mentiras...\n",
      "mentiras -> 0.008243697640580194 -> 0.9917563023594198\n",
      "odio -> 0.020479341668647066 -> 0.9795206583313529\n",
      "El porcentaje de suicidarte es de: 0.62\n",
      "Estoy odiando éstos ataques de ansiedad....\n",
      "ansiedad -> -0.0008974142933766416 -> 1.0008974142933766\n",
      "ataques -> 0.01086109418434944 -> 0.9891389058156506\n",
      "estoy -> -0.004200413642365675 -> 1.0042004136423657\n",
      "estos -> 0.030609599375566177 -> 0.9693904006244338\n",
      "odiando -> 0\n",
      "El porcentaje de suicidarte es de: 0.35\n",
      "~Ser un asco de persona, sentirse patético por pensarlo, quedar hecho lerda por todo.~\n",
      "quedar -> -0.0021765271989693247 -> 1.0021765271989693\n",
      "persona -> 0.022577986319504362 -> 0.9774220136804956\n",
      "patetico -> -0.02624374875227886 -> 1.0262437487522789\n",
      "asco -> -0.015767408402041383 -> 1.0157674084020414\n",
      "pensarlo -> 0.001243254639483382 -> 0.9987567453605166\n",
      "lerda -> 0\n",
      "sentirse -> 0.011431625782847732 -> 0.9885683742171523\n",
      "hecho -> 0.013048686217047134 -> 0.9869513137829529\n",
      "El porcentaje de suicidarte es de: 0.03\n",
      "Heridas abiertas...\n",
      "heridas -> 0.01807639029686925 -> 0.9819236097031308\n",
      "abiertas -> 0\n",
      "El porcentaje de suicidarte es de: 0.42\n",
      "@yoongidoblxs Ternuras!!!!!\n",
      "ternuras -> 0\n",
      "yoongidoblxs -> 0\n",
      "El porcentaje de suicidarte es de: 0\n",
      "@kenroVlogs ._. https://t.co/xrkgwXuoGt\n",
      "https -> 0.0004830377929110341 -> 0.999516962207089\n",
      "kenrovlogs -> 0\n",
      "xrkgwxuogt -> 0\n",
      "El porcentaje de suicidarte es de: 0.01\n",
      "Como se hace para ignorar ese sentimiento de que algo anda terriblemente mal?\n",
      "hace -> -0.013582733876035036 -> 1.013582733876035\n",
      "ignorar -> 0.03110990621541987 -> 0.9688900937845801\n",
      "sentimiento -> 0.0066431579048185085 -> 0.9933568420951815\n",
      "terriblemente -> 0\n",
      "El porcentaje de suicidarte es de: 0.29\n",
      "Estoy destrozada... Y no lo digo metafóricamente... Literalmente estoy hecha mierda y ya no sé que más hacer...\n",
      "hacer -> -0.026349726151693176 -> 1.0263497261516932\n",
      "destrozada -> -0.024130662717595897 -> 1.024130662717596\n",
      "estoy -> -0.004200413642365675 -> 1.0042004136423657\n",
      "metaforicamente -> 0\n",
      "literalmente -> 0.018340819215154625 -> 0.9816591807848454\n",
      "mierda -> 0.008338760218180141 -> 0.9916612397818199\n",
      "digo -> 0.019596691351750906 -> 0.9804033086482491\n",
      "hecha -> 0\n",
      "El porcentaje de suicidarte es de: -0.05\n",
      "Sueños rotos...\n",
      "rotos -> 0.01581815186187896 -> 0.984181848138121\n",
      "suenos -> 0.011245101046313266 -> 0.9887548989536867\n",
      "El porcentaje de suicidarte es de: 0.59\n",
      "Dolor...\n",
      "dolor -> 0.007518675786592155 -> 0.9924813242134078\n",
      "El porcentaje de suicidarte es de: 0.36\n",
      "Desconfianza..\n",
      "desconfianza -> 0.008274157610173916 -> 0.9917258423898261\n",
      "El porcentaje de suicidarte es de: 0.39\n",
      "Tristeza...\n",
      "tristeza -> 0.005474705693870874 -> 0.9945252943061291\n",
      "El porcentaje de suicidarte es de: 0.27\n",
      "Decepción...\n",
      "decepcion -> 0.011695916896940162 -> 0.9883040831030598\n",
      "El porcentaje de suicidarte es de: 0.53\n",
      "No es normal... Sentirse tan mal con uno mismo...\n",
      "mismo -> 0.005966875595845522 -> 0.9940331244041545\n",
      "sentirse -> 0.011431625782847732 -> 0.9885683742171523\n",
      "El porcentaje de suicidarte es de: 0.41\n",
      "Tengo miedo de estar en una relación... No quiero sentir ese dolor punzante en el pecho por una traición, no otra vez.\n",
      "punzante -> 0\n",
      "traicion -> 0.022263250794445355 -> 0.9777367492055546\n",
      "otra -> 0.02396360344795312 -> 0.9760363965520469\n",
      "quiero -> 0.0568789449880569 -> 0.9431210550119431\n",
      "miedo -> 0.015788547686724996 -> 0.984211452313275\n",
      "estar -> 0.0212984298928518 -> 0.9787015701071482\n",
      "pecho -> -0.013658547142199495 -> 1.0136585471421995\n",
      "dolor -> 0.007518675786592155 -> 0.9924813242134078\n",
      "sentir -> -0.029088876591956137 -> 1.0290888765919561\n",
      "tengo -> 0.0227931054672581 -> 0.9772068945327419\n",
      "relacion -> -0.0168541203327095 -> 1.0168541203327095\n",
      "El porcentaje de suicidarte es de: 0.47\n",
      "Que triste es sentirse excluida de mi propia familia :c\n",
      "propia -> 0.042597228850986824 -> 0.9574027711490132\n",
      "familia -> 0.019436861598329358 -> 0.9805631384016706\n",
      "excluida -> -0.011674480755154093 -> 1.011674480755154\n",
      "sentirse -> 0.011431625782847732 -> 0.9885683742171523\n",
      "El porcentaje de suicidarte es de: 0.65\n",
      "Además de estar nublado como me gusta, hoy fue un lindo día :)...\n",
      "nublado -> -0.01685923365050712 -> 1.0168592336505071\n",
      "ademas -> -0.023920672291239953 -> 1.02392067229124\n",
      "estar -> 0.0212984298928518 -> 0.9787015701071482\n",
      "El porcentaje de suicidarte es de: -0.31\n",
      "RT @Juanii_Bello98: La verdad que hoy fue un dia de mierda😑\n",
      "verdad -> 0.0024147144845301227 -> 0.9975852855154699\n",
      "bello -> 0\n",
      "juanii -> 0\n",
      "mierda -> 0.008338760218180141 -> 0.9916612397818199\n",
      "El porcentaje de suicidarte es de: 0.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @eldiariodedross: Perturbadross https://t.co/icLei7XrAo\n",
      "perturbadross -> 0\n",
      "https -> 0.0004830377929110341 -> 0.999516962207089\n",
      "xrao -> 0\n",
      "iclei -> 0\n",
      "eldiariodedross -> 0\n",
      "El porcentaje de suicidarte es de: 0.00\n",
      "💔🌻\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c9ff76cdd38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" -> \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontador\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mnue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"El porcentaje de suicidarte es de: 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "def normalize(arr):\n",
    "    total = 0\n",
    "    for j in arr:\n",
    "        total += float(j)\n",
    "    return total\n",
    "\n",
    "\n",
    "import tweepy\n",
    "consumer_key = \"HSzv7R6stxA2MPYRgpaACZ5ef\"\n",
    "consumer_secret = \"4EAMUKdFYV6xcugD67buBt75yVftYWZyNerxyWY8CI0Fbp2Y3e\"\n",
    "access_token = \"432589386-UfuQTmjwQHGB7TKIZwEK1QB4vqU3zQFRnTvjoOsr\"\n",
    "access_token_secret = \"jWgl243HcySFZ0z19zz3154IIUqU9YDzKqCJga9CKPhmn\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "tt = 100\n",
    "promSui = 0\n",
    "results = api.user_timeline(id=\"MaruManriquez18\", count=tt)\n",
    "contador = 1\n",
    "for tweet in results:\n",
    "    contador=contador+1\n",
    "    print(tweet.text)\n",
    "    valid_examples = limpia_impureza(tweet.text)\n",
    "\n",
    "    acum=0\n",
    "    for j in valid_examples:\n",
    "        try:\n",
    "            i = int(dictionary[j])\n",
    "            valid_word = reverse_dictionary[i]\n",
    "            top_k = 7  # number of nearest neighbors\n",
    "            nearest = (final_embeddings[i, :]).argsort()[1:top_k + 1]\n",
    "            total = []\n",
    "            for k in xrange(top_k):\n",
    "                low_dim_embs = final_embeddings[nearest[k], :]\n",
    "                promedio = 0\n",
    "                for j in low_dim_embs:\n",
    "                    promedio+=float(j)\n",
    "                promedio = promedio/128\n",
    "                total.append(promedio)\n",
    "            norm = normalize(total)\n",
    "            acum+=norm\n",
    "            print(valid_word + \" -> \" + str(norm) + \" -> \"+str(1-norm))\n",
    "        except KeyError:\n",
    "            print(j + \" -> \" + str(0))\n",
    "    ws.update_cell(contador, 1, tweet.text)\n",
    "    nue = (acum/len(valid_examples))*100\n",
    "    if(nue == 0.0):\n",
    "        print(\"El porcentaje de suicidarte es de: 0\")\n",
    "        ws.update_cell(contador, 2, 0)\n",
    "    else:\n",
    "        val = y = ((1 / (1 + math.exp(-nue)))-0.5)*2\n",
    "        promSui += val\n",
    "        print(\"El porcentaje de suicidarte es de: %.2f\" % val)\n",
    "        ws.update_cell(contador, 2, val)\n",
    "        \n",
    "promSui /= tt\n",
    "ws2.update_cell(2,1,promSui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
